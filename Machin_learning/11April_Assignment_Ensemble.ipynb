{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Ensemble Techniques "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Answer 1\n",
    "### Ensemble Technique in machine learning:\n",
    "An ensemble technique in machine learning involves combining the predictions of multiple models to improve the overall performance of the system. \n",
    "\n",
    "#### There are several types of ensemble techniques, including:\n",
    "\n",
    "- Bagging: This technique involves training multiple instances of the same model on different subsets of the training data, and then combining their predictions through averaging or voting.\n",
    "\n",
    "- Boosting: This technique involves sequentially training a series of weak models, where each subsequent model is trained on the data that was misclassified by the previous model. The final prediction is a weighted sum of the individual model predictions.\n",
    "\n",
    "- Stacking: This technique involves training multiple models and using their predictions as input features for a higher-level model, which then makes the final prediction.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Why are ensemble techniques used in machine learning? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2\n",
    "\n",
    "Ensemble techniques are used in machine learning for several reasons, including:\n",
    "\n",
    "-  Improved performance:\n",
    "      By combining the predictions of multiple models, ensemble methods can reduce errors and improve accuracy.\n",
    "\n",
    "- Robustness: Ensemble techniques can make a machine learning system more robust to outliers and noise in the data. By using multiple models, ensemble methods can reduce the impact of individual data points that may be anomalous or mislabeled.\n",
    "\n",
    "- Generalization: Ensemble techniques can improve the generalization ability of a machine learning system by reducing overfitting. By using multiple models, ensemble methods can capture a wider range of patterns and relationships in the data, and avoid over-relying on any particular feature or attribute.\n",
    "\n",
    "- Flexibility: Ensemble techniques can be applied to a wide range of machine learning problems, including classification, regression, and clustering. They can also be used with different types of models and algorithms, allowing for greater flexibility in model selection.\n",
    "   \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 3 \n",
    "# Bagging:\n",
    "Bagging, short for Bootstrap Aggregating, is an ensemble technique in machine learning that involves training multiple instances of the same model on different subsets of the training data, and then combining their predictions through averaging or voting.\n",
    "\n",
    "\n",
    "- The key idea behind bagging is to reduce the variance of the model by introducing randomness into the training process. By training multiple models on different subsets of the data, bagging can help to reduce the impact of individual data points that may be anomalous or mislabeled."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 4\n",
    "\n",
    "## Boosting:\n",
    "Boosting is an ensemble technique in machine learning that involves sequentially training a series of weak models, where each subsequent model is trained on the data that was misclassified by the previous model. The final prediction is a weighted sum of the individual model predictions.\n",
    "\n",
    "- The key idea behind boosting is to focus on the data points that are difficult to classify, and to give them more weight in the training process. By training a series of weak models on the misclassified data, boosting can iteratively improve the accuracy of the model.\n",
    "\n",
    "- Boosting can be applied to a wide range of machine learning problems, including classification, regression, and ranking. It is often used with decision trees, as they are weak models that can be easily combined into a boosted ensemble.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 5 \n",
    "\n",
    "#### Ensemble techniques in machine learning offer several benefits, including:\n",
    "\n",
    "\n",
    "-  Improved performance:\n",
    "      By combining the predictions of multiple models, ensemble methods can reduce errors and improve accuracy.\n",
    "\n",
    "- Robustness: Ensemble techniques can make a machine learning system more robust to outliers and noise in the data. By using multiple models, ensemble methods can reduce the impact of individual data points that may be anomalous or mislabeled.\n",
    "\n",
    "- Generalization: Ensemble techniques can improve the generalization ability of a machine learning system by reducing overfitting. By using multiple models, ensemble methods can capture a wider range of patterns and relationships in the data, and avoid over-relying on any particular feature or attribute.\n",
    "\n",
    "- Flexibility: Ensemble techniques can be applied to a wide range of machine learning problems, including classification, regression, and clustering. They can also be used with different types of models and algorithms, allowing for greater flexibility in model selection.\n",
    "\n",
    "- State-of-the-art performance: \n",
    "   Ensemble techniques have been shown to consistently improve the performance of machine learning systems, and are widely used in practice to achieve state-of-the-art results on a variety of tasks.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. Are ensemble techniques always better than individual models?\n",
    "\n",
    "\n",
    "## Answer 6 \n",
    "Ensemble techniques are not always better than individual models.The performance of an ensemble method depends on several factors, including the quality of the individual models, the diversity of the models, and the way in which they are combined.\n",
    "\n",
    "- In some cases, an individual model may be sufficient to achieve high accuracy on a given task, and ensemble methods may not offer significant improvements.In other cases, the individual models may be highly diverse, making it difficult to combine their predictions in a meaningful way.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Q7. How is the confidence interval calculated using bootstrap?\n",
    "\n",
    "## Answer 7 :\n",
    "The confidence interval is a statistical measure of the precision or uncertainty of an estimate.\n",
    "\n",
    "- In bootstrap, a large number of resamples are generated by randomly sampling the original data with replacement. Then, a statistic of interest (such as the mean, median, or standard deviation) is calculated on each resample. The distribution of these statistics is used to estimate the true distribution of the statistic for the population.\n",
    "\n",
    "###   To calculate the confidence interval using bootstrap, the following steps can be followed:\n",
    "\n",
    "\n",
    "- Generate a large number of bootstrap resamples (usually 1000 or more) by randomly sampling the original data with replacement.\n",
    "\n",
    "- Calculate the statistic of interest (such as the mean or standard deviation) on each bootstrap resample.\n",
    "\n",
    "- Calculate the standard error of the statistic by taking the standard deviation of the bootstrap statistics.\n",
    "\n",
    "- Calculate the lower and upper bounds of the confidence interval using the formula:\n",
    "\n",
    "- Lower Bound = Estimate - (Z * Standard Error)\n",
    "- Upper Bound = Estimate + (Z * Standard Error)\n",
    "\n",
    "-  -  where Estimate is the estimate of the statistic on the original data, Z is the z-score corresponding to the desired confidence level (e.g., 1.96 for a 95% confidence interval), and Standard Error is the standard error calculated in step 3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Q8. How does bootstrap work and What are the steps involved in bootstrap?\n",
    "\n",
    "\n",
    "Bootstrap is a resampling technique in statistics and machine learning that involves creating new datasets by drawing samples with replacement from an original dataset. The main idea behind bootstrap is to simulate the sampling distribution of a statistic or model parameter from the available data.\n",
    "\n",
    "##### The steps involved in bootstrap are as follows:\n",
    "\n",
    "- Sample with replacement: Randomly draw a sample of size n (the same as the original dataset) from the original dataset, with replacement. This means that each sample can contain duplicate observations from the original dataset.\n",
    "\n",
    "- Calculate the statistic of interest: Compute the statistic of interest (such as the mean, standard deviation, or regression coefficient) on the resampled dataset.\n",
    "\n",
    "- Repeat: Repeat steps 1 and 2 B times (where B is typically a large number, e.g., 1000 or more), creating B bootstrap samples and B corresponding estimates of the statistic.\n",
    "\n",
    "- Analyze the distribution of bootstrap estimates: Examine the distribution of the B estimates of the statistic, which approximates the sampling distribution of the statistic. This distribution can be used to estimate confidence intervals, test hypotheses, or make other statistical inferences.\n",
    "\n",
    "\n",
    "The key idea behind bootstrap is that the resampled datasets are similar to the original dataset, and can be used to estimate the variability of the statistic or model parameter of interest. By repeating this process many times, a distribution of estimates is obtained, which can be used to calculate confidence intervals or make other statistical inferences\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 9\n",
    "To estimate the 95% confidence interval for the population mean height of trees using bootstrap, we can follow these steps:\n",
    "\n",
    "- Resample the original sample of 50 trees with replacement to generate a large number (e.g., 1000) of bootstrap samples.\n",
    "- Calculate the mean height of each bootstrap sample.\n",
    "- Calculate the standard deviation of the bootstrap means.\n",
    "- Calculate the 2.5th and 97.5th percentiles of the distribution of bootstrap means to obtain the lower and upper bounds of the 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% confidence interval: [14.87, 14.99]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# original sample\n",
    "sample = np.array([15.1, 14.9, 14.5, 15.2, 14.8, 15.3, 15.0, 14.6, 15.1, 15.2, 15.3, 15.2, 14.8, 15.0, 14.9, 15.1, 14.7, 15.0, 14.9, 15.3, 14.6, 14.7, 15.0, 14.9, 14.5, 15.4, 15.1, 15.2, 15.0, 14.7, 14.9, 15.0, 14.6, 14.8, 14.7, 15.1, 14.9, 14.7, 14.8, 14.5, 15.1, 14.8, 14.7, 15.2, 15.1, 14.9, 15.1, 15.0, 14.6, 15.1, 14.7])\n",
    "\n",
    "# number of bootstrap samples\n",
    "B = 1000\n",
    "\n",
    "# bootstrap resampling\n",
    "bootstrap_means = []\n",
    "for i in range(B):\n",
    "    bootstrap_sample = np.random.choice(sample, size=len(sample), replace=True)\n",
    "    bootstrap_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_means.append(bootstrap_mean)\n",
    "\n",
    "# standard deviation of bootstrap means\n",
    "bootstrap_std = np.std(bootstrap_means)\n",
    "\n",
    "# lower and upper bounds of the confidence interval\n",
    "lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "print(\"95% confidence interval: [{:.2f}, {:.2f}]\".format(lower_bound, upper_bound))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
