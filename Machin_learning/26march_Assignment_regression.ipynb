{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e3d4e-65bc-4e67-b57e-7e93f17f9cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afa37139-6e0e-4e2d-8bc5-f4a43853062c",
   "metadata": {},
   "source": [
    "# Answer 1\n",
    "\n",
    "## Simple Linear Regression:\n",
    "   Simple linear regression is a statistical method used to establish a linear relationship between two variables. In this type of regression, one independent variable is used to predict the value of a dependent variable. For example, a simple linear regression model could be used to predict a person's weight (dependent variable) based on their height (independent variable).\n",
    "\n",
    "## Multiple Regression :\n",
    "  Multiple linear regression, on the other hand, is used when two or more independent variables are used to predict the value of a dependent variable. For example, a multiple linear regression model could be used to predict a person's salary (dependent variable) based on their level of education and years of experience (independent variables).\n",
    "\n",
    "\n",
    "#### Example of Simple Linear Regression:\n",
    "\n",
    "Suppose we want to predict the sales of a product based on the amount spent on advertising. In this case, the amount spent on advertising is the independent variable, and the sales of the product are the dependent variable. A simple linear regression model could be used to estimate the relationship between these two variables, and we could use this model to predict sales based on a given amount spent on advertising.\n",
    "\n",
    "\n",
    "#### Example of Multiple Linear Regression:\n",
    "\n",
    "Suppose we want to predict the price of a house based on several variables, such as the size of the house, the number of bedrooms, and the age of the house. In this case, we have three independent variables (size, bedrooms, and age), and the price of the house is the dependent variable. A multiple linear regression model could be used to estimate the relationship between these variables, and we could use this model to predict the price of a house based on its size, number of bedrooms, and age.\n",
    "\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2df3ea-036c-4bec-a2bc-10bbab6dc24e",
   "metadata": {},
   "source": [
    "# Answer 2\n",
    "\n",
    "### Linear regression makes several assumptions about the data, including:\n",
    "\n",
    "- **Linearity**: \n",
    "   There is a linear relationship between the independent and dependent variables.\n",
    "\n",
    "- **Homoscedasticity:**\n",
    "    The variance of the errors (residuals) is constant across all levels of the independent variable.\n",
    "\n",
    "- **Independence:**\n",
    "  The errors (residuals) are independent of each other.\n",
    "\n",
    "- **Normality:**\n",
    "   The errors (residuals) follow a normal distribution.\n",
    "\n",
    "- **No multicollinearity:**\n",
    "\n",
    "   The independent variables are not highly correlated with each other.\n",
    "   \n",
    "#### All Assumptions are shown in a image given below\n",
    "\n",
    "<img src=\"Asssumption of liner_regression.jpg\" alt=\"alt text\" />\n",
    "\n",
    "\n",
    "\n",
    "-------\n",
    "\n",
    "\n",
    "## To check whether these assumptions hold in a given dataset, you can use several methods: \n",
    "\n",
    "- Plotting the data: \n",
    "   You can create scatter plots to visualize the relationship between the independent and dependent variables. If the relationship is linear, this assumption is met.\n",
    "\n",
    "- Checking residuals: \n",
    "   You can create a residual plot to check for homoscedasticity. If the residuals are spread evenly around zero and do not have a cone shape, this assumption is met.\n",
    "\n",
    "- Durbin-Watson test: \n",
    "  This test is used to check for independence of errors (residuals). The Durbin-Watson test statistic ranges from 0 to 4, with a value of 2 indicating no autocorrelation.\n",
    "\n",
    "- Normal probability plot: \n",
    "   This plot can be used to check for normality. If the residuals follow a straight line, this assumption is met.\n",
    "\n",
    "- Variance inflation factor (VIF): \n",
    "   This statistic is used to check for multicollinearity. A VIF greater than 5 indicates a high degree of correlation between independent variables.\n",
    "   \n",
    "---------\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee83651f-c76f-427f-ade5-dda110f3edc1",
   "metadata": {},
   "source": [
    "# Answer 3\n",
    "\n",
    "## Interpreting the slope and intercept in a linear regression model \n",
    "\n",
    "In a linear regression model, the slope and intercept represent the relationship between the independent variable(s) and the dependent variable.\n",
    "\n",
    "- ### Intercept \n",
    "The intercept represents the value of the dependent variable when all independent variables are zero. In other words, it is the value of the dependent variable when there is no input from the independent variable(s). \n",
    "\n",
    "\n",
    "\n",
    "- ### Slope\n",
    "   The slope represents the change in the dependent variable for a unit change in the independent variable(s). It tells us how much the dependent variable changes for a one-unit increase in the independent variable(s).\n",
    "\n",
    "\n",
    "**y = mx+c**\n",
    "\n",
    "where \n",
    "c = Intercept\n",
    "\n",
    "x = independent variable\n",
    "\n",
    "m = slope\n",
    "\n",
    "y = dependent varoiba;e\n",
    "   \n",
    "\n",
    "\n",
    "## Example  based on the above statement\n",
    "\n",
    "###  Example 1.\n",
    "Data were collected on the depth of a dive of penguins and the duration of\n",
    "the dive. The following linear model is a fairly good summary of the data, where t is the\n",
    "duration of the dive in minutes and d is the depth of the dive in yards. The equation for\n",
    "the model is d t = + 0.015 2.915\n",
    "\n",
    "\n",
    "- Interpret the slope:\n",
    "\n",
    "If the duration of the dive increases by 1 minute, we predict the\n",
    "depth of the dive will increase by approximately 2.915 yards.\n",
    "\n",
    "- Interpret the intercept. \n",
    " If the duration of the dive is 0 seconds, then we predict the\n",
    "depth of the dive is 0.015 yards.\n",
    "\n",
    "\n",
    "- Comments: \n",
    " The interpretation of the intercept doesn’t make sense in the real world. It\n",
    "isn’t reasonable for the duration of a dive to be near t = 0 , because that’s too short for a\n",
    "dive. If data with x-values near zero wouldn’t make sense, then usually the interpretation\n",
    "of the intercept won’t seem realistic in the real world. It is, however, acceptable (even\n",
    "required) to interpret this as a coefficient in the model. \n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc82c9-b3f3-4289-9b6a-722ffc52af48",
   "metadata": {},
   "source": [
    "# Answer 4\n",
    "### The concept of gradient descent and importance of using it in  the machine learning \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1585f081-7bd9-4e91-87ef-a37aa48c1d99",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "Gradient descent is an optimization algorithm used in machine learning to find the optimal solution for a given problem by minimizing the cost function.\n",
    "The cost function is a measure of how well the model is performing on the training data\n",
    "\n",
    "### How is it used in machine learning?\n",
    "\n",
    "- Gradient descent is used to train many types of machine learning models, including linear regression, logistic regression, and neural networks.\n",
    "\n",
    "- During training, the algorithm updates the model parameters in the direction of the negative gradient of the cost function, which measures the difference between the predicted output of the model and the true output.\n",
    "\n",
    "- The learning rate is a hyperparameter that controls the size of the updates at each iteration, and it is often tuned using cross-validation.\n",
    "\n",
    "- Batch gradient descent computes the gradient of the cost function over the entire training dataset, while stochastic gradient descent computes the gradient over a single training example at a time.\n",
    "\n",
    "- Mini-batch gradient descent is a compromise between batch and stochastic gradient descent, where the gradient is computed over a small batch of training examples at a time.\n",
    "\n",
    "- Gradient descent can be used for both supervised learning, where the goal is to predict a target variable given input features, and unsupervised learning, where the goal is to learn patterns or structure in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e04ed2-98e5-4fa5-9b87-bb22d1f2da72",
   "metadata": {},
   "source": [
    "## Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d640c-bbf5-4f51-9c2a-f92204624df7",
   "metadata": {},
   "source": [
    "# Answerr 5\n",
    "### Multiple Linear Regression \n",
    "Multiple linear regression is a statistical modeling technique used to study the relationship between a dependent variable and **two or more independent variables.**\n",
    "\n",
    "\n",
    "In multiple linear regression, the relationship between the dependent variable and the independent variables is modeled using a linear equation of the form:\n",
    "\n",
    "y = β0 + β1x1 + β2x2 + ... + βpxp + ε\n",
    "  \n",
    "  \n",
    "  where y is the dependent variable, β0 is the intercept term, β1, β2, ..., βp are the coefficients of the independent variables x1, x2, ..., xp, respectively, and ε is the error term.\n",
    "  \n",
    "  \n",
    "  The goal of multiple linear regression is to estimate the values of the coefficients β0, β1, β2, ..., βp that minimize the sum of squared errors between the predicted and actual values of the dependent variable.\n",
    "  \n",
    "\n",
    "## It is differ from the Simple linear Regression \n",
    "\n",
    "####  1 Complexity \n",
    "  multiple linear regression allows us to model more complex relationships between the dependent variable and the independent variables. It can account for the effects of multiple variables on the dependent variable and can help identify which variables are most strongly associated with the dependent variable.\n",
    "\n",
    "#### 2  .Independent Varibales \n",
    "  Simple linear regression involves only one independent variable, while multiple linear regression involves two or more independent variables.  \n",
    "\n",
    "\n",
    "#### 3 nterpretation of the coefficients.\n",
    "  In simple linear regression, the coefficient represents the change in the dependent variable for a one-unit change in the independent variable. In multiple linear regression, the interpretation is more complex, as the coefficient represents the change in the dependent variable for a one-unit change in the corresponding independent variable, holding all other independent variables constant.\n",
    "  \n",
    "  \n",
    "---\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5ea375-5a2a-4c73-b2a0-5bbf5c18e795",
   "metadata": {},
   "source": [
    "# Answer 6\n",
    "\n",
    "## Concept of the Multicollinearity in Multiple Regression \n",
    "   Multicollinearity is a common issue that can arise in multiple linear regression when two or more independent variables are highly correlated with each other. This can cause problems in the model, such as unstable or unreliable estimates of the regression coefficients, and can make it difficult to identify the true effects of each independent variable on the dependent variable\n",
    "   \n",
    "   \n",
    "### Detection of the Multicollinearity\n",
    "#### Method 1 \n",
    "\n",
    "-  Calculate the correlation matrix of the idependent variables \n",
    "- Look high corrlation (i.e., correlation coefficients close to +1 or -1) \n",
    "\n",
    "\n",
    "#### Method 2\n",
    "\n",
    "-  Another way to detect multicollinearity is to calculate the variance inflation factor (VIF) for each independent variable.which measures how much the variance of the estimated coefficient is inflated due to multicollinearity.\n",
    "-  A VIF value greater than 5 or 10 is generally considered a sign of multicollinearity.\n",
    "\n",
    "\n",
    "### To address multicollinearity, we can take several approaches:\n",
    "\n",
    "- Remove one of the highly correlated independent variables from the model. This approach may be appropriate if the independent variables are conceptually similar or if one of the variables is less important than the others.\n",
    "\n",
    "- Combine the highly correlated independent variables into a single variable. For example, we could compute the average or principal component of the highly correlated variables.\n",
    "\n",
    "- Use regularization techniques such as ridge regression or Lasso regression. These techniques add a penalty term to the cost function of the model, which encourages the coefficients to be small and helps to stabilize the estimates even when there is multicollinearity.\n",
    "\n",
    "\n",
    "- Collect more data to reduce the impact of multicollinearity. With more data, the estimates of the regression coefficients will become more stable and reliable, even in the presence of multicollinearity.\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efbeb7-d914-4b16-98c8-df269df4e8fc",
   "metadata": {},
   "source": [
    "# Answer 7\n",
    "## Polynomial Regression Model\n",
    "- (Non-linear relationship between dependent and Indepenent Variables)\n",
    "\n",
    "Polynomial regression is a type of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial function of x. The polynomial function is given by:\n",
    "\n",
    "\n",
    "y = β0 + β1x + β2x^2 + ... + βnx^n + ε\n",
    "\n",
    "where y is the dependent variable, x is the independent variable, β0, β1, β2, ..., βn are the coefficients of the polynomial terms, ε is the error term, and n is the degree of the polynomial.\n",
    "\n",
    "## The key difference between polynomial regression and linear regression are\n",
    "\n",
    "1 The relationship between the dependent variable and the independent variable is modeled as a linear function of x, whereas in polynomial regression, the relationship can be modeled as a non-linear function of x.\n",
    "\n",
    "2 Polynomial regression can be useful when the relationship between the dependent variable and the independent variable is not linear.but can be better approximated by a polynomial function. For example, in some cases, a quadratic or cubic function may provide a better fit to the data than a linear function.\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cfc19a-00a4-4834-9217-4ff3c4e881dc",
   "metadata": {},
   "source": [
    "# Answer 8 \n",
    "\n",
    "\n",
    "### Advantages of polynomial regression compared to linear regression:\n",
    "\n",
    "- Flexibility:\n",
    "\n",
    "  Polynomial regression can model non-linear relationships between the dependent and independent variables, which linear regression cannot.\n",
    "\n",
    "- Higher accuracy: \n",
    "\n",
    "    If the true relationship between the variables is non-linear, polynomial regression can provide a better fit to the data than linear regression, leading to higher prediction accuracy.\n",
    "\n",
    "- Interpretation: \n",
    "\n",
    "   Polynomial regression allows for easy interpretation of the impact of each degree of the polynomial on the dependent variable, which can help in understanding the relationship between the variables\n",
    "   \n",
    "   -----\n",
    "   \n",
    "### Disadvantages of polynomial regression compared to linear regression:\n",
    "- Overfitting: \n",
    "  \n",
    "  As the degree of the polynomial increases, the model can become more complex and overfit the data, leading to poor performance on new, unseen data.\n",
    "\n",
    "- Computational complexity: \n",
    "\n",
    "   Polynomial regression can be computationally expensive, especially for higher degrees of the polynomial.\n",
    "\n",
    "- Extrapolation:\n",
    "\n",
    "   Extrapolation can be risky with polynomial regression, as the model may not accurately predict values outside the range of the data.\n",
    "\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "### Situations in which polynomial regression may be preferred:\n",
    "\n",
    "**1 Non-linear relationships:**\n",
    "\n",
    "      If the relationship between the dependent and independent variables is non-linear, polynomial regression can provide a better fit than linear regression.\n",
    "     \n",
    "**2 Limited data:** \n",
    "  \n",
    "     In cases where there is limited data, polynomial regression can help to capture the relationship between the variables more accurately than linear regression.\n",
    "     \n",
    "**3 High prediction accuracy:**\n",
    "\n",
    "     If the goal is to achieve high prediction accuracy, and the true relationship between the variables is non-linear, polynomial regression may be preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7057286f-84c5-441c-9338-8110c8362e70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
