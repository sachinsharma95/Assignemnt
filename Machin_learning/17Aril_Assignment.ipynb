{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copyright (c) @sachin shamrma 2023\n",
    "\n",
    "# <center> Assignment Based on Gradient Boosting\n",
    "\n",
    "\n",
    "---------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1\n",
    "\n",
    "\n",
    "### Gradient Boosting Regression\n",
    "Gradient Boosting Regression is a machine learning algorithm used for regression problems that involves sequentially adding weak regression models to an ensemble, with each new model attempting to correct the errors made by the previous model.\n",
    "- . It is a form of boosting algorithm that works by iteratively fitting simple regression models to the residual errors of the previous model.\n",
    "\n",
    "####  In Gradient Boosting Regression, the objective is to minimize the mean squared error (MSE) between the predicted values and the actual values. \n",
    "####  The algorithm uses a loss function (such as the mean squared error) to measure the error of each weak model, and then uses gradient descent to find the optimal parameters for each model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate sample data\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "y = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20])\n",
    "\n",
    "# Split data into training set and test set\n",
    "train_idx = np.random.choice(len(X), size=8, replace=False)\n",
    "test_idx = np.array(list(set(range(len(X))) - set(train_idx)))\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        y_pred = np.full(y.shape, np.mean(y))\n",
    "        for i in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            residuals = y - y_pred\n",
    "            tree.fit(X, residuals)\n",
    "            update = tree.predict(X)\n",
    "            y_pred += self.learning_rate * update\n",
    "            self.estimators.append(tree)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(len(X))\n",
    "        for tree in self.estimators:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 94.253907161691\n",
      "R-squared score: -93.253907161691\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train model\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "model.fit(X_train.reshape(-1, 1), y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = model.predict(X_test.reshape(-1, 1))\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean squared error:\", mse)\n",
    "print(\"R-squared score:\", r2)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters\n",
    "\n",
    "\n",
    "## Answer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=GradientBoostingRegressor(), n_jobs=-1,\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.01, 0.1, 1],\n",
       "                         &#x27;max_depth&#x27;: [2, 3, 4],\n",
       "                         &#x27;n_estimators&#x27;: [20, 49, 100]},\n",
       "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=GradientBoostingRegressor(), n_jobs=-1,\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.01, 0.1, 1],\n",
       "                         &#x27;max_depth&#x27;: [2, 3, 4],\n",
       "                         &#x27;n_estimators&#x27;: [20, 49, 100]},\n",
       "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=GradientBoostingRegressor(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.01, 0.1, 1],\n",
       "                         'max_depth': [2, 3, 4],\n",
       "                         'n_estimators': [20, 49, 100]},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "\n",
    "# Generate sample data\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "y = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20])\n",
    "\n",
    "# Split data into training set and test set\n",
    "train_idx = np.random.choice(len(X), size=8, replace=False)\n",
    "test_idx = np.array(list(set(range(len(X))) - set(train_idx)))\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "        'n_estimators': [20, 49,100],\n",
    "        'learning_rate': [0.01, 0.1, 1],\n",
    "        'max_depth': [2, 3, 4]\n",
    "    }\n",
    "\n",
    "\n",
    "# Grsdient boosting\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "\n",
    "# grid_search = GridSearchCV(model, param_grid=param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train.reshape(-1, 1), y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 1, 'max_depth': 2, 'n_estimators': 20}\n",
      "Mean squared error: 4.0\n",
      "R-squared score: 0.0\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test.reshape(-1, 1))\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Mean squared error:\", mse)\n",
    "print(\"R-squared score:\", r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "### At learinng rate alpha = 1 and max_depth = 2, n_estimator = 20\n",
    "- MSE = 4\n",
    "- Root Squareed Error = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4\n",
    "- In Gradient Boosting, a weak learner is a model that is only slightly better than random guessing.\n",
    "-A weak learner is a simple model that has low predictive power on its own, but can still contribute to the overall accuracy of the final model when combined with other weak learners.\n",
    "\n",
    "- In the context of Gradient Boosting for regression, a common weak learner is a decision tree with a single split, also known as a decision stump. This type of model can only make predictions based on a single feature and a single threshold, which is not very powerful."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 5\n",
    "The intuition behind Gradient Boosting is to iteratively add weak learners to a model in a way that each new learner tries to correct the mistakes made by the previous learners, gradually improving the overall accuracy of the model.\n",
    "\n",
    "- The name \"Gradient Boosting\" comes from the fact that the algorithm uses the gradient, or the direction of steepest descent, to update the parameters of the model at each iteration. The gradient represents the change in the loss function with respect to the parameters of the model, and it is used to determine the direction in which the parameters should be updated to reduce the loss.\n",
    "\n",
    "### Conclusion\n",
    "In summary, the intuition behind Gradient Boosting is to build an ensemble of weak learners that work together to gradually reduce the error of the model by correcting the mistakes made by the previous learners, using the gradient to update the parameters and improve the accuracy of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 6\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners by adding them one at a time in a sequential manner, where each new learner tries to correct the mistakes made by the previous learners.\n",
    "\n",
    "####  The algorithm follows these steps:\n",
    "- First, a base model is trained on the training data. This model can be any simple model, such as a decision tree with a single split, also known as a decision stump.\n",
    "\n",
    "- Next, the residuals, or errors, of the base model are calculated for each training sample. The residuals are the differences between the true values and the predicted values of the base model.\n",
    "\n",
    "- A new weak learner, such as another decision stump, is trained on the residuals of the previous model. This weak learner is trained to predict the residual errors of the previous model, rather than the original target values.\n",
    "\n",
    "- The new model is added to the ensemble, and the predictions of the entire ensemble are updated by adding the predictions of the new model multiplied by a small learning rate, which controls the contribution of the new model to the final prediction.\n",
    "\n",
    "- Steps 3 and 4 are repeated for a specified number of iterations, each time training a new weak learner on the residuals of the previous model, adding it to the ensemble, and updating the predictions of the ensemble.\n",
    "\n",
    "The final prediction of the Gradient Boosting ensemble is the sum of the predictions of all the weak learners, each multiplied by a weight that represents its contribution to the final prediction. The weights are determined based on the performance of each weak learner on the training data, with more weight given to the models that perform better.\n",
    "\n",
    "\n",
    "###### Note:-\n",
    "By iteratively adding weak learners to the ensemble, the Gradient Boosting algorithm is able to gradually reduce the errors in the predictions and improve the accuracy of the final model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 7\n",
    "The mathematical intuition behind Gradient Boosting involves the following steps:\n",
    "\n",
    "#### Define the loss function:\n",
    " The first step is to define a loss function that measures the difference between the true values and the predicted values of the model. The most common loss function used in regression problems is the mean squared error (MSE), which measures the average of the squared differences between the true and predicted values.\n",
    "\n",
    "#### Train a base model: \n",
    " A base model, such as a decision tree with a single split, is trained on the training data to predict the target values. This model serves as the starting point for the Gradient Boosting algorithm.\n",
    "\n",
    "#### Calculate the residuals: \n",
    "The residuals, or errors, of the base model are calculated for each training sample by subtracting the predicted values of the base model from the true values.\n",
    "\n",
    "#### Train a new model:\n",
    " A new model, such as another decision tree, is trained on the residuals of the previous model. This new model is trained to predict the residuals, rather than the original target values.\n",
    "\n",
    "#### Update the predictions: \n",
    "The predictions of the entire ensemble are updated by adding the predictions of the new model multiplied by a small learning rate, which controls the contribution of the new model to the final prediction.\n",
    "\n",
    " - Repeat: Steps 3 to 5 are repeated for a specified number of iterations, with each new model trained on the residuals of the previous model and added to the ensemble.\n",
    "\n",
    "### Final prediction: \n",
    "The final prediction of the Gradient Boosting ensemble is the sum of the predictions of all the models in the ensemble, each multiplied by a weight that represents its contribution to the final prediction. The weights are determined based on the performance of each model on the training data, with more weight given to the models that perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
