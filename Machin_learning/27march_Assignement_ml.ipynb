{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f607e340-fbd1-40e0-bd80-a3d9f6e57d25",
   "metadata": {},
   "source": [
    "## <center> R-squared , Adjusted R- Squared & (MAE, RMSE, MSE )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c5448e-610e-459a-96a2-e10401ef3da7",
   "metadata": {},
   "source": [
    "# Answer 1\n",
    "## R-squared:\n",
    "- R-squared is a measure of how well the regression model fits the data.\n",
    "- R-squared is a statistical measure that represents the proportion of variation in the dependent variable (the outcome variable) that is explained by the independent variable(s) (predictor variable(s)) in a linear regression model.\n",
    "\n",
    "\n",
    "## R-squared is calculated \n",
    "R-squared is calculated by dividing the explained variance (i.e., the variance in the dependent variable that is explained by the independent variable(s)) by the total variance in the dependent variable. \n",
    "\n",
    "\n",
    "### In mathematical terms, R-squared is calculated as follows:\n",
    "\n",
    "R-squared = Explained variance / Total variance\n",
    "\n",
    "where:\n",
    "\n",
    "Explained variance = sum of squares of the regression (SSR)\n",
    "\n",
    "Total variance = sum of squares total (SST)\n",
    "\n",
    "\n",
    "- SSR is the sum of the squared differences between the predicted values and the mean of the dependent variable\n",
    "- SST is the sum of the squared differences between the actual values and the mean of the dependent variabl\n",
    "\n",
    "R-squared is a useful measure for evaluating the goodness of fit of a linear regression model. A higher R-squared value indicates that the model explains more of the variance in the dependent variable, which suggests that the model is a better fit for the data.\n",
    "\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc4151-448f-4e35-baa9-223cda9e1e55",
   "metadata": {},
   "source": [
    "# Answer 2\n",
    "## Adjusted R- Squared\n",
    "Adjusted R-squared is a modified version of R-squared that takes into account the number of independent variables used in a linear regression model. Unlike R-squared, which only considers the proportion of variance in the dependent variable that is explained by the independent variable(s), adjusted R-squared adjusts for the number of independent variables in the model.\n",
    "\n",
    "#### Adjusted R-squared is calculated using the following formula:\n",
    "\n",
    "Adjusted R-squared = **1 - [(1 - R-squared) * (n - 1) / (n - k - 1)]**\n",
    "\n",
    "where n is the sample size and k is the number of independent variables in the model.\n",
    "\n",
    "\n",
    "- The adjusted R-squared value ranges from 0 to 1, \n",
    "- with higher values indicating a better fit of the model to the data.\n",
    "\n",
    "### Difference between Adjusted R - Squared and R- Squared\n",
    "\n",
    "- Regular R-squared only measures the proportion of variance in the dependent variable that is explained by the independent variable(s), while adjusted R-squared takes into account the number of independent variables used in the model.\n",
    "\n",
    "- Regular R-squared does not penalize the addition of unnecessary independent variables to the model, while adjusted R-squared adjusts for overfitting by penalizing the inclusion of irrelevant independent variables.\n",
    "\n",
    "- Regular R-squared values will always increase as more independent variables are added to the model, while adjusted R-squared values will only increase if the additional independent variables improve the model's fit.\n",
    "\n",
    "- Regular R-squared can be misleading when comparing models with different numbers of independent variables, while adjusted R-squared provides a more accurate measure of the goodness of fit when comparing models with different numbers of independent variables.\n",
    "\n",
    "- Regular R-squared is widely used and is often reported in research papers, while adjusted R-squared is less commonly used but provides a more nuanced measure of the performance of a linear regression model.\n",
    "\n",
    "\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b6cceb-6111-4840-8ab5-8e081608c981",
   "metadata": {},
   "source": [
    "# Answer 3 \n",
    "\n",
    "### Adjusted R-squared is more appropriate to use in the following situations:\n",
    "\n",
    "1 When comparing linear regression models with different numbers of independent variables. Adjusted R-squared is a better measure of the goodness of fit of a model when comparing models with different numbers of independent variables because it adjusts for the number of independent variables in the model.\n",
    "\n",
    "2 When there are many independent variables in the model. If there are many independent variables in the model, regular R-squared may overestimate the goodness of fit of the model, while adjusted R-squared can provide a more accurate measure of the model's performance by adjusting for the number of independent variables.\n",
    "\n",
    "\n",
    "3 When there is a risk of overfitting. If the model includes irrelevant or unnecessary independent variables, regular R-squared may overestimate the model's performance, while adjusted R-squared can provide a more conservative estimate by penalizing the inclusion of irrelevant variables.\n",
    "\n",
    "4 When the sample size is small. If the sample size is small, regular R-squared may overestimate the goodness of fit of the model, while adjusted R-squared can provide a more accurate measure of the model's performance by adjusting for the number of independent variables and the small sample size.\n",
    "\n",
    "\n",
    "### In Summary \n",
    "Adjusted R-squared is more appropriate to use when comparing models with different numbers of independent variables, when there are many independent variables in the model, when there is a risk of overfitting, and when the sample size is small.\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef852134-d4a8-4db2-8e62-15c4887eaf3a",
   "metadata": {},
   "source": [
    "# Answer 4\n",
    "\n",
    "\n",
    "## 1 Root Mean Squared Error (RMSE):\n",
    "RMSE is a commonly used metric for evaluating the accuracy of a regression model. It measures the average magnitude of the errors between the predicted and actual values of the dependent variable. RMSE is calculated by taking the square root of the mean of the squared errors.\n",
    "The formula for RMSE is:\n",
    "RMSE = sqrt(sum of (predicted value - actual value)^2 / n)\n",
    "\n",
    "where n is the number of observations in the sample.\n",
    "\n",
    "RMSE is a measure of the standard deviation of the residuals, or prediction errors, of the regression model. A lower RMSE indicates that the model has better predictive performance.\n",
    "\n",
    "## 2 Mean Squared Error (MSE):\n",
    "MSE is another commonly used metric for evaluating the accuracy of a regression model. It measures the average of the squared errors between the predicted and actual values of the dependent variable. MSE is calculated by taking the mean of the squared errors.\n",
    "The formula for MSE is:\n",
    "MSE = sum of (predicted value - actual value)^2 / n\n",
    "\n",
    "where n is the number of observations in the sample.\n",
    "\n",
    "MSE is also a measure of the variability of the residuals of the regression model. A lower MSE indicates that the model has better predictive performance.\n",
    "\n",
    "## 3 Mean Absolute Error (MAE):\n",
    "MAE is a metric for evaluating the accuracy of a regression model that measures the average magnitude of the errors between the predicted and actual values of the dependent variable. Unlike MSE and RMSE, MAE does not square the errors, which makes it less sensitive to outliers.\n",
    "The formula for MAE is:\n",
    "MAE = sum of abs(predicted value - actual value) / n\n",
    "\n",
    "where n is the number of observations in the sample.\n",
    "\n",
    "MAE is a measure of the average absolute difference between the predicted and actual values of the dependent variable. A lower MAE indicates that the model has better predictive performance.\n",
    "\n",
    "\n",
    "\n",
    "RMSE, MSE, and MAE are all measures of the accuracy of a regression model that quantify the average magnitude or variability of the errors between the predicted and actual values of the dependent variable. RMSE and MSE are more sensitive to outliers and penalize larger errors more heavily, while MAE is less sensitive to outliers and penalizes all errors equally.\n",
    "\n",
    "\n",
    "\n",
    "-----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a19d15-ab75-4489-9a7d-72a10d9aad85",
   "metadata": {},
   "source": [
    "# Answer 5\n",
    "##  The advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
    "\n",
    "\n",
    "### Advantages of RMSE:\n",
    "\n",
    "- RMSE is sensitive to outliers, making it useful in situations where outliers need to be identified and removed.\n",
    "- RMSE provides a measure of the spread of errors and can be useful in identifying the variance of the error distribution.\n",
    "- RMSE is differentiable, which makes it useful in optimization problems.\n",
    "\n",
    "### Disadvantages of RMSE:\n",
    "\n",
    "- RMSE gives higher weight to larger errors, which may not always be desirable.\n",
    "- RMSE does not have an intuitive interpretation because it is in the same units as the target variable.\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "### Advantages of MSE:\n",
    "\n",
    "- MSE is sensitive to outliers and provides a measure of the spread of errors.\n",
    "- MSE is differentiable, making it useful in optimization problems.\n",
    "\n",
    "### Disadvantages of MSE:\n",
    "\n",
    "- Like RMSE, MSE gives higher weight to larger errors, which may not always be desirable.\n",
    "- MSE is not in the same units as the target variable, making it difficult to interpret.\n",
    "\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "### Advantages of MAE:\n",
    "\n",
    "- MAE is robust to outliers and gives equal weight to all errors, making it useful in situations where outliers are present or where all errors should be treated equally.\n",
    "- MAE is in the same units as the target variable, making it easy to interpret.\n",
    "\n",
    "### Disadvantages of MAE:\n",
    "\n",
    "- MAE does not provide a measure of the spread of errors, which can be useful in some situations.\n",
    "- MAE is not differentiable at all points, which can make it difficult to use in optimization problems.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0592aed9-2a06-4ebc-b53b-29e4ec45d552",
   "metadata": {},
   "source": [
    "# Answer 6\n",
    "\n",
    "## Lasso (Least Absolute Shrinkage and Selection Operator) \n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used in regression analysis to reduce the complexity of a model by shrinking the coefficients of some of the features to zero. \n",
    "\n",
    "# 1 How Lasso regularization works:\n",
    "\n",
    "- Lasso regularization adds a penalty term to the regression objective function, which is proportional to the sum of the absolute values of the coefficients.\n",
    "- The penalty term encourages the coefficients of some of the features to become exactly zero, resulting in a sparse model where only a subset of the features is used in the final model.\n",
    "- The amount of regularization is controlled by a hyperparameter, lambda, which determines the strength of the penalty term.\n",
    "\n",
    "\n",
    "# 2 How Lasso differs from Ridge regularization:\n",
    "\n",
    "- Ridge regularization adds a penalty term to the regression objective function, which is proportional to the sum of the squares of the coefficients.\n",
    "- The penalty term encourages all of the coefficients to become small, but none of them to be exactly zero, resulting in a model where all of the features are used to some extent.\n",
    "- The amount of regularization is controlled by a hyperparameter, alpha, which determines the strength of the penalty term.\n",
    "\n",
    "\n",
    "# 3 When to use Lasso regularization:\n",
    "\n",
    "\n",
    "- Lasso regularization is more appropriate when there are many features in the dataset and some of them may be irrelevant or redundant.\n",
    "- Lasso can perform feature selection, as it tends to shrink the coefficients of some features to exactly zero, resulting in a sparse model where only a subset of the features is used in the final model.\n",
    "\n",
    "- Lasso regularization may be less appropriate when all of the features are important and should be included in the final model, or when there is a high degree of multicollinearity among the features, as it may be difficult to identify which features to include and which to exclude. In such cases, Ridge regularization may be more appropriate.\n",
    "\n",
    "\n",
    "------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe6e1d-9a25-417e-8823-9ae5e947a2b0",
   "metadata": {},
   "source": [
    "# Answer 7\n",
    "Regularized linear models such as Ridge regression and Lasso regression are used to prevent overfitting in machine learning. These models add a penalty term to the loss function during training to encourage the model to find a balance between fitting the training data and avoiding overly complex models.\n",
    "\n",
    "\n",
    "### Here's an example of how regularized linear models can help prevent overfitting:\n",
    "\n",
    "Suppose you have a dataset of housing prices with various features such as the number of bedrooms, the square footage of the house, and the location. You want to build a linear regression model to predict the housing prices based on these features.\n",
    "\n",
    "\n",
    "\n",
    "Without any regularization, the linear regression model may fit the training data too closely and overfit, resulting in poor performance on new, unseen data. To prevent overfitting, you can use a regularized linear model such as Ridge regression or Lasso regression.\n",
    "\n",
    "\n",
    "\n",
    "For example, let's say you decide to use Lasso regression with an L1 penalty term. Lasso regression adds a penalty term proportional to the sum of the absolute values of the coefficients to the loss function during training. This encourages the model to find coefficients that are exactly zero for some features, effectively removing them from the model and reducing complexity.\n",
    "\n",
    "\n",
    "As a result, the Lasso regression model may perform better than a regular linear regression model without any regularization, because it can effectively filter out noise and irrelevant features that might otherwise lead to overfitting.\n",
    "\n",
    "\n",
    "In summary, regularized linear models such as Lasso regression can help prevent overfitting by adding a penalty term to the loss function during training that encourages the model to find a balance between fitting the training data and avoiding overly complex models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83af78d-71a8-4ddb-b4c5-0705d2229b0c",
   "metadata": {},
   "source": [
    "# Answer 8 \n",
    "\n",
    "\n",
    "\n",
    "Although regularized linear models such as Ridge regression and Lasso regression are effective in preventing overfitting and improving generalization performance, they may not always be the best choice for regression analysis due to several limitations. \n",
    "\n",
    "\n",
    "# Some limitations of regularized linear models:\n",
    "\n",
    "\n",
    "# 1 Limited flexibility: \n",
    "Regularized linear models assume a linear relationship between the features and the target variable. This assumption may not always hold true, and if the relationship is highly nonlinear, then regularized linear models may not be the best choice.\n",
    "\n",
    "# 2 Feature selection bias:\n",
    "Lasso regression may perform feature selection by setting some coefficients to zero. However, this may lead to a biased selection of features, and some important features may be missed.\n",
    "\n",
    "# 3 Sensitivity to outliers:\n",
    "Regularized linear models are sensitive to outliers, and a few extreme data points can have a significant impact on the model's predictions. Outliers may have a larger impact on regularized linear models compared to non-regularized models.\n",
    "\n",
    "# 4  Choice of hyperparameters:\n",
    "Regularized linear models have hyperparameters that need to be chosen carefully, such as the regularization strength parameter. The performance of the model can be highly dependent on the choice of hyperparameters, which can be time-consuming and computationally expensive to tune.\n",
    "\n",
    "# 5 Interpretability: \n",
    "Regularized linear models are often criticized for their lack of interpretability. Since the coefficients are shrunk towards zero, it can be difficult to interpret the importance of each feature in the model.\n",
    "\n",
    "\n",
    "## they may not always be the best choice for regression analysis.\n",
    "\n",
    "### Regularized linear models may not always be the best choice for regression analysis due to several reasons:\n",
    "\n",
    "\n",
    "## Nonlinear relationships:\n",
    "Regularized linear models assume a linear relationship between the features and the target variable. However, if the relationship is highly nonlinear, then linear models may not capture the complexity of the data and may not be the best choice. In such cases, nonlinear models such as decision trees or neural networks may be more suitable.\n",
    "\n",
    "## Interpretability: \n",
    "Regularized linear models are often criticized for their lack of interpretability. Since the coefficients are shrunk towards zero, it can be difficult to interpret the importance of each feature in the model. This may not be desirable in situations where interpretability is critical, such as in healthcare or finance.\n",
    "\n",
    "## Feature selection bias: \n",
    "  Lasso regression may perform feature selection by setting some coefficients to zero. However, this may lead to a biased selection of features, and some important features may be missed. This can be especially problematic in situations where all features may be relevant and important.\n",
    "\n",
    "## Sensitivity to outliers: \n",
    "Regularized linear models are sensitive to outliers, and a few extreme data points can have a significant impact on the model's predictions. Outliers may have a larger impact on regularized linear models compared to non-regularized models.\n",
    "\n",
    "\n",
    "---------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3375fb11-8ffd-4103-9be7-722c38da9caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caa4f5bf-d92d-4193-a2af-a484305bbf71",
   "metadata": {},
   "source": [
    "**Q9. You are comparing the performance of two regression models using different evaluation metrics. \n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better \n",
    "performer, and why? Are there any limitations to your choice of metric?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8104856-f9cd-49c9-914f-557bba2032e2",
   "metadata": {},
   "source": [
    "# Answer 9 \n",
    "\n",
    "\n",
    "Given :\n",
    "Model A has an RMSE of 10 <br>\n",
    "Model B has an MAE of 8\n",
    "\n",
    "\n",
    "## Solution  with Explanation\n",
    "\n",
    "The choice of which model is the better performer would depend on the specific problem at hand and the priorities of the stakeholder.\n",
    "\n",
    "If the stakeholder is more concerned with larger errors (i.e., outliers), then Model A with an RMSE of 10 may be more appropriate. On the other hand, if the stakeholder is more concerned with the average error, then Model B with an MAE of 8 may be more appropriate.\n",
    "\n",
    "\n",
    "### Limitations to both the RMSE and MAE as evaluation metrics\n",
    "\n",
    "Yes, there are limitations to both the RMSE and MAE as evaluation metrics, which are as follows:\n",
    "\n",
    "- Neither RMSE nor MAE considers the direction of errors. That is, they treat overestimations and underestimations equally, even though in some cases, one type of error may be more important than the other.\n",
    "\n",
    "- Both metrics assume that errors are normally distributed. However, in some cases, the errors may have a non-normal distribution, and alternative evaluation metrics may be more appropriate.\n",
    "\n",
    "- RMSE gives higher weight to larger errors than MAE. In some cases, this may be desirable, but in others, it may not accurately reflect the stakeholder's priorities.\n",
    "\n",
    "- MAE is not differentiable at zero, which can cause problems for some optimization algorithms.\n",
    "\n",
    "- Both RMSE and MAE do not provide any information about the goodness of fit or the predictive power of the regression model. In some cases, alternative evaluation metrics like R-squared or adjusted R-squared may be more appropriate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac00de6-ce93-4dae-b34e-d66c0a6ff2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f6cc643-9081-4b8c-b2e5-e589c98cf8c7",
   "metadata": {},
   "source": [
    "**Q10. You are comparing the performance of two regularized linear models using different types of \n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B \n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the \n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization \n",
    "method?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc269da3-7c51-48f4-ad38-8689e0b0a322",
   "metadata": {},
   "source": [
    "# Answer 10 \n",
    "\n",
    "\n",
    "Ridge regularization and Lasso regularization are both used to prevent overfitting in linear regression models by adding a penalty term to the cost function. Ridge regularization adds the sum of squared values of the coefficients multiplied by a regularization parameter to the cost function, while Lasso regularization adds the sum of absolute values of the coefficients multiplied by a regularization parameter.\n",
    "\n",
    "\n",
    "### Explanation \n",
    "\n",
    "If the stakeholder is more concerned with reducing the variance of the model and avoiding overfitting, then Model A with Ridge regularization may be more appropriate. Ridge regularization tends to shrink the coefficients of less important features towards zero, but it does not perform feature selection. On the other hand, if the stakeholder is more concerned with feature selection and obtaining a more interpretable model, then Model B with Lasso regularization may be more appropriate. Lasso regularization tends to set the coefficients of less important features exactly to zero, effectively removing them from the model.\n",
    "\n",
    "### Yes, there are trade-offs and limitations to the choice of regularization method. Some of these include:\n",
    "\n",
    "- **Bias-Variance Trade-off:** \n",
    "Regularization methods aim to balance the bias-variance trade-off in the model. However, the choice of the regularization parameter affects this balance. A high regularization parameter value may lead to an underfitting model, while a low regularization parameter value may lead to an overfitting model.\n",
    "\n",
    "- **Selection of Regularization Parameter:** \n",
    "\n",
    "   Choosing the regularization parameter requires tuning to achieve the best performance of the model. However, the choice of the regularization parameter is not always straightforward, and it may require the use of cross-validation or other techniques.\n",
    "\n",
    "- **Computational Complexity:** \n",
    "  Regularization methods may introduce additional computational complexity to the model, especially when the number of features is large. This is because the regularization term requires computing the sum of squares or absolute values of the coefficients.\n",
    "\n",
    "- **Sensitivity to Correlated Features:** \n",
    "  Lasso regularization, in particular, may have difficulty selecting features when there are correlated features in the data. This is because it tends to choose only one of the correlated features and sets the others to zero.\n",
    "\n",
    "- **Limited Feature Selection:**\n",
    "   Regularization methods do not guarantee optimal feature selection in all cases. Some important features may be mistakenly excluded or less important features may be included.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4158b4a-a144-41ca-8375-3b312c3fed4a",
   "metadata": {},
   "source": [
    "Thank You ::\n",
    "    \n",
    "    Notes By : Sachin Sharma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b7f08-1e09-444e-a6c9-dc6b38ac48a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
