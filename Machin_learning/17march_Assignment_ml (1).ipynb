{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec97d66-0c8d-4a98-a22f-ab1a8ea82f44",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5effad86-7a59-4239-a2ce-998d891e1592",
   "metadata": {},
   "source": [
    " # Answer 1\n",
    "    \n",
    "### Missing Values:\n",
    "Missing values refer to the absence of data for one or more features in a dataset. The reasons for missing values can vary, such as data entry errors, data corruption, or the data simply not being collected.\n",
    "\n",
    "##### Essential to handle because:\n",
    "It is essential to handle missing values because they can impact the accuracy and performance of machine learning models.\n",
    "\n",
    "\n",
    "Missing values can lead to biased or inaccurate predictions, affect the quality of the results, and potentially lead to incorrect conclusions.\n",
    "\n",
    "\n",
    "\n",
    "#### Some machine learning algorithms that can handle missing values include:\n",
    "\n",
    "- __Decision Trees:__\n",
    "     Decision Trees can handle missing values by creating surrogate splits.\n",
    "\n",
    "- **Random Forest:** \n",
    "\n",
    "    Random Forest can handle missing values by creating surrogate splits.\n",
    "\n",
    "- **K-Nearest Neighbors:**\n",
    "\n",
    "   K-Nearest Neighbors can handle missing values by imputing the missing values with the mean or median of the feature.\n",
    "\n",
    "- **Naive Bayes:** \n",
    "\n",
    "   Naive Bayes can handle missing values by ignoring the missing values during the calculation of probabilities.\n",
    "\n",
    "- **Support Vector Machines:** \n",
    "\n",
    "    Support Vector Machines can handle missing values by using a technique called \"soft imputation.\"\n",
    "\n",
    "- **Gradient Boosted Trees:** \n",
    "\n",
    "  Gradient Boosted Trees can handle missing values by treating missing values as a separate category during the training process.\n",
    "\n",
    "- **Neural Networks:** \n",
    "\n",
    "   Neural Networks can handle missing values by using techniques such as mean imputation, median imputation, or using specific models designed to handle missing data.\n",
    "   \n",
    "   \n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d3a6f0-332b-48f0-b106-14786f12ca2e",
   "metadata": {},
   "source": [
    "# Answer 2\n",
    "####  Some techniques used to handle missing data. With example of each with python code.\n",
    "\n",
    "\n",
    "#### 1 Deletion: \n",
    "In this technique, we remove the missing values from the dataset. There are three types of deletion: listwise deletion, pairwise deletion, and complete case deletion. Listwise deletion removes entire rows with missing values, pairwise deletion removes pairs of values that are missing, and complete case deletion removes cases with any missing value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45157a1e-e152-42c1-a086-2a786c329504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B\n",
      "0  1.0   6.0\n",
      "1  2.0   7.0\n",
      "4  5.0  10.0\n"
     ]
    }
   ],
   "source": [
    "# Deletion \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "data = {'A': [1, 2, None, 4, 5], 'B': [6, 7, 8, None, 10]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c58bff-4fa1-4713-95ff-1ffca4def7b0",
   "metadata": {},
   "source": [
    "### Mean/median imputation:\n",
    "In this technique, the missing values are replaced with the mean or median of the available data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c977c3-f502-4df0-9a2d-388b3c095e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A      B\n",
      "0  1.0   6.00\n",
      "1  2.0   7.00\n",
      "2  3.0   8.00\n",
      "3  4.0   7.75\n",
      "4  5.0  10.00\n"
     ]
    }
   ],
   "source": [
    "#### 2 Mean/Mode/Median Imputation:\n",
    "import pandas as pd\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "data = {'A': [1, 2, None, 4, 5], 'B': [6, 7, 8, None, 10]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# replace missing values with the mean of the column\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a576ec-d4a9-4753-a88c-c0ab478dab7a",
   "metadata": {},
   "source": [
    "###  Mode imputation: \n",
    "In this technique, the missing values are replaced with the mode of the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e09903c-a04b-4511-b666-2a4913466375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B\n",
      "0  1.0   6.0\n",
      "1  2.0   7.0\n",
      "2  1.0   8.0\n",
      "3  4.0   6.0\n",
      "4  5.0  10.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# mode imputation \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "data = {'A': [1, 2, None, 4, 5], 'B': [6, 7, 8, None, 10]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# replace missing values with the mode of the column\n",
    "df.fillna(df.mode().iloc[0], inplace=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d0d3e7-c450-4729-92c1-5ed7d2ce15fa",
   "metadata": {},
   "source": [
    "###  Interpolation technique\n",
    "In this case, the missing values are replaced with the values obtained by connecting the nearest known values with a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd469df3-766f-46d1-a52d-e0f55b9ef095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B\n",
      "0  1.0   6.0\n",
      "1  2.0   7.0\n",
      "2  3.0   8.0\n",
      "3  4.0   9.0\n",
      "4  5.0  10.0\n"
     ]
    }
   ],
   "source": [
    "# Interpolation technique\n",
    "import pandas as pd\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "data = {'A': [1, 2, None, 4, 5], 'B': [6, None, 8, None, 10]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# interpolate missing values using linear interpolation\n",
    "df.interpolate(method='linear', inplace=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e399fdf6-860c-4240-a4ef-5153e708ba79",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fb428b-3e6f-44ca-8635-0ae48781bc88",
   "metadata": {},
   "source": [
    "# Answer 3\n",
    "### Imbalance Data:\n",
    "Imbalanced data refers to a situation in which the number of observations in one class or category of the dependent variable is much higher or much lower than the number of observations in other classes or categories. \n",
    "\n",
    "It is a situation where the classes are not represented equally in the data, and one or more classes have a much smaller number of observations compared to other classes.\n",
    "\n",
    "\n",
    "- ####  Impact \n",
    "Imbalanced data can lead to biased models and incorrect predictions, especially in binary classification problems.\n",
    "\n",
    "\n",
    "### If imbalanced data is not handled, the following consequences may arise:\n",
    "\n",
    "- **Bias towards the majority class:** \n",
    "\n",
    "   The model may be biased towards the majority class, leading to an over-representation of the majority class in the predictions.\n",
    "\n",
    "- **Poor generalization:** \n",
    "\n",
    "  The model may not generalize well to new data if the class distribution in the new data is different from the class distribution in the training data.\n",
    "\n",
    "- **Lower predictive performance:** \n",
    "\n",
    "    The model may have lower accuracy, precision, recall, and F1 score for the minority class, resulting in lower predictive performance overall.\n",
    "\n",
    "- **Incorrect ranking:**  \n",
    "\n",
    "   The model may incorrectly rank the instances in the minority class, leading to a higher false negative rate and missing important instances.\n",
    "\n",
    "\n",
    "###  How can we handle  ::\n",
    "\n",
    "To avoid these consequences, it is essential to handle imbalanced data by using techniques such as oversampling, undersampling, cost-sensitive learning, and data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3051c7-ed31-4209-b7d8-8ed584cb33e7",
   "metadata": {},
   "source": [
    "# Answer 4\n",
    "\n",
    "### Upsampling and DownSampling\n",
    "\n",
    "\n",
    "#### Down-sampling: \n",
    "   In this technique, the majority class is randomly reduced to match the number of observations in the minority class. This can result in a smaller dataset but can help balance the class distribution.\n",
    "\n",
    "#### Up-sampling: \n",
    "In this technique, the minority class is randomly duplicated to match the number of observations in the majority class. This can result in a larger dataset but can help balance the class distribution.\n",
    "\n",
    "\n",
    "\n",
    "#### Here are some examples of when up-sampling and down-sampling may be required:\n",
    "\n",
    "- Example of the upsampling:\n",
    "\n",
    "   Up-sampling may be required when the minority class is under-represented, and the model is not correctly identifying instances from that class. For example, in fraud detection, the number of fraudulent transactions may be much lower than non-fraudulent transactions. In this case, up-sampling the minority class can help improve the model's performance.\n",
    "   \n",
    "   \n",
    "- Down Sampling:       \n",
    "       Down-sampling may be required when the majority class is over-represented, and the model is biased towards that class. For example, in medical diagnosis, the number of healthy patients may be much higher than the number of patients with a disease. In this case, down-sampling the majority class can help balance the class distribution and improve the model's performance.\n",
    "\n",
    "---------\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9efbe0-0b1b-4175-9608-aaaa0a9c8a4f",
   "metadata": {},
   "source": [
    "# Answer 5\n",
    "\n",
    "### Data Augmentation\n",
    "Data augmentation is a technique used to increase the size of the training dataset by applying various transformations to the existing data. It is commonly used in machine learning and deep learning to improve the performance of models by increasing the amount and diversity of data available for training. Data augmentation can be applied to various types of data, such as images, text, and time-series data. The most common data augmentation techniques include flipping, cropping, rotation, scaling, and adding noise\n",
    "\n",
    "\n",
    "### SMOTE\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a popular data augmentation technique used to handle imbalanced data. It creates synthetic samples of the minority class by interpolating new examples between existing minority class samples. The SMOTE algorithm works by selecting one minority class observation at random and then finding its k nearest minority class neighbors. Synthetic examples are then generated by taking a linear combination of the minority class observation and its k nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ba56e3-1e5e-4769-bc73-d00a000edc93",
   "metadata": {},
   "source": [
    "# Answr 6 \n",
    "## Outlier\n",
    "\n",
    "Outliers in a dataset are data points that are significantly different from other observations in the dataset. They can be identified by their extreme values that are either too high or too low compared to the rest of the data. Outliers can occur due to various reasons such as data entry errors, measurement errors, or extreme events, and can have a significant impact on the statistical analysis of the dataset.\n",
    "\n",
    "### Handling outliers is essential for the following reasons:\n",
    "\n",
    "- Outliers can significantly affect the mean and standard deviation of the dataset, making them unreliable as measures of central tendency and variability. Removing or adjusting outliers can help improve the accuracy of these measures.\n",
    "\n",
    "- Outliers can affect the distribution of the data, making it non-normal. Many statistical models assume normality, and outliers can violate this assumption.\n",
    "- Removing or adjusting outliers can help improve the validity of the statistical models.\n",
    "\n",
    "- Outliers can affect the correlation between variables in the dataset, making it difficult to interpret the relationship between them. Removing or adjusting outliers can help improve the accuracy of correlation analysis.\n",
    "\n",
    "- Outliers can have a significant impact on the predictive performance of machine learning models. Many machine learning algorithms are sensitive to outliers, and removing or adjusting them can help improve the model's performance.\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0861571-10d2-420a-bf54-7f8b23cb94b1",
   "metadata": {},
   "source": [
    "# Answrr 7 \n",
    "### While Working on the project , The some technique to handle the missing the data.\n",
    "\n",
    "#### 1 Deletion:\n",
    "\n",
    "In this technique, we remove the missing values from the dataset. There are three types of deletion: listwise deletion, pairwise deletion, and complete case deletion. Listwise deletion removes entire rows with missing values, pairwise deletion removes pairs of values that are missing, and complete case deletion removes cases with any missing value.\n",
    "\n",
    "#### 2 Mean/median imputation:\n",
    "\n",
    "In this technique, the missing values are replaced with the mean or median of the available data.\n",
    "\n",
    "#### 3 Interpolation technique\n",
    "In this case, the missing values are replaced with the values obtained by connecting the nearest known values with a straight line.\n",
    "\n",
    "#### 4 Expectation-Maximization (EM): \n",
    "\n",
    "EM is an iterative method that estimates the missing data values and model parameters simultaneously. It is commonly used in data analysis, especially in cases where the data is missing not at random.\n",
    "\n",
    "#### 5 K-Nearest Neighbor (KNN): \n",
    "\n",
    "In this technique, missing values are imputed by identifying the K nearest neighbors to the observation with missing data and using the average value of the nearest neighbors to fill in the missing data.\n",
    "\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4841e9a9-97e6-4606-a82c-8434ee68a47b",
   "metadata": {},
   "source": [
    "#### Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?\n",
    "\n",
    "# Answer 8\n",
    "There are several strategies that can be used to determine if the missing data is missing at random or if there is a pattern to the missing data. \n",
    "\n",
    "#### Some of the commonly used strategies are:\n",
    "\n",
    "- Visualizations: \n",
    "\n",
    "  Visualizations can be used to identify patterns in the missing data. Missing data can be represented graphically, such as through heat maps, histograms, and scatter plots, to identify whether missing data is clustered in particular regions or distributed randomly.\n",
    "\n",
    "- Statistical tests:\n",
    "\n",
    "  Statistical tests can be used to determine whether the missing data is missing at random or is systematically related to other variables in the dataset. Tests such as Little's MCAR test and the chi-square test can be used to determine whether the missing data is missing completely at random or not.\n",
    "\n",
    " - Imputation methods: \n",
    " \n",
    "   Imputation methods can be used to examine the relationship between missing data and other variables in the dataset. For example, imputing missing data using regression imputation can help identify whether the missing data is related to other variables in the dataset.\n",
    "   \n",
    "- Machine Learning: \n",
    "\n",
    "  Machine learning algorithms can be used to identify patterns in the missing data. Techniques such as clustering and association rule mining can be used to identify patterns and relationships in the missing data.\n",
    "\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a1a026-bfc0-4b2f-ab69-fa4842e01196",
   "metadata": {},
   "source": [
    "####  Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "\n",
    "\n",
    "# Answer 9\n",
    "When working with an imbalanced dataset, where one class has significantly fewer samples than the other class, the performance of a machine learning model can be biased towards the majority class. \n",
    "Therefore, it is essential to evaluate the performance of the model using appropriate metrics that consider the class imbalance.\n",
    "\n",
    "\n",
    "#### Here are some strategies that can be used to evaluate the performance of machine learning models on imbalanced datasets:\n",
    "\n",
    "-  Cost-Sensitive Learning:\n",
    "\n",
    "    Cost-sensitive learning is a technique that assigns different costs to different types of classification errors based on the class imbalance. This approach can help to train a model that is more sensitive to the minority class and reduces the false-negative rate.\n",
    "\n",
    "-  Sampling Techniques: \n",
    "\n",
    "    Sampling techniques like over-sampling and under-sampling can be used to balance the dataset by either increasing the minority class samples or decreasing the majority class samples. This can help to improve the model's performance on the minority class.\n",
    "    \n",
    "-  Confusion Matrix: \n",
    "\n",
    "   A confusion matrix can be used to evaluate the performance of the model. It provides information about the number of true positive, false positive, true negative, and false negative predictions. Using this, we can calculate performance metrics like precision, recall, F1 score, and accuracy.\n",
    "\n",
    "-------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e4c88d-c19e-441d-bd5e-97c515870a91",
   "metadata": {},
   "source": [
    "###  Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\n",
    "\n",
    "\n",
    "# Answer 10 \n",
    "\n",
    "When dealing with an imbalanced dataset, where the majority class has significantly more samples than the minority class, it can be beneficial to balance the dataset to improve the performance of the machine learning model. \n",
    "\n",
    "#### Here are some methods that can be used to balance the dataset and down-sample the majority class:\n",
    "\n",
    "- Random Under-Sampling: \n",
    "\n",
    "   In random under-sampling, we randomly remove samples from the majority class to balance the dataset. This can be a quick and easy method to balance the dataset, but it may result in the loss of important information.\n",
    "\n",
    "-  Cluster Centroids:\n",
    "\n",
    "   In cluster centroids, we use clustering algorithms to identify centroids of the majority class and replace each cluster with its centroid. This method can help to preserve the structure of the majority class and improve the performance of the model.\n",
    "\n",
    "- Tomek Links: \n",
    "\n",
    "  Tomek Links are pairs of samples that are close to each other but belong to different classes. We can remove the majority class samples that form Tomek links to balance the dataset.\n",
    "\n",
    "- SMOTE: \n",
    "   SMOTE is a popular method for balancing imbalanced datasets. In SMOTE, we create synthetic minority class samples by interpolating between the minority class samples. This can help to increase the size of the minority class and balance the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0a49ce-4624-4445-93d8-f51d3e0c4bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8309afe1-d13d-4505-8047-c01e2efc12d6",
   "metadata": {},
   "source": [
    "# Answer 11 \n",
    "\n",
    "When dealing with an imbalanced dataset where the minority class has a low percentage of occurrences, we can use various techniques to balance the dataset and up-sample the minority class. \n",
    "\n",
    "### Here are some methods that can be used to up-sample the minority class:\n",
    "\n",
    "- Random Over-Sampling: \n",
    "\n",
    "   In random over-sampling, we randomly duplicate samples from the minority class until we have a balanced dataset. This method can be quick and easy, but it may result in overfitting and the generation of redundant data.\n",
    "\n",
    "- SMOTE: \n",
    "  SMOTE is a popular method for balancing imbalanced datasets. In SMOTE, we create synthetic minority class samples by interpolating between the minority class samples. This can help to increase the size of the minority class and balance the dataset.\n",
    "\n",
    "- ADASYN: \n",
    "\n",
    "   ADASYN is a variation of SMOTE that creates synthetic minority class samples based on the density distribution of the dataset. This can help to create more diverse synthetic samples and improve the performance of the model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d7753-eb13-4f64-9fb5-24374e8c8ce4",
   "metadata": {},
   "source": [
    "#### python code \n",
    "\n",
    "```\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
