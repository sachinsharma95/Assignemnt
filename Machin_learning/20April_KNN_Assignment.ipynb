{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# <center> Assignment based on KNN."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1\n",
    "# KNN Algorithm:\n",
    "- The K-Nearest Neighbors (KNN) algorithm is a supervised machine learning algorithm used for classification and regression tasks.\n",
    "-  It is a non-parametric method that uses the distances between data points to predict the class or value of a new data point.\n",
    "- KNN works by finding the K nearest neighbors to the new data point in the training dataset and then taking a majority vote (in classification) or averaging (in regression) of their labels or values to determine the prediction for the new data point"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2\n",
    "\n",
    "- The choice of the value of K is an important hyperparameter in KNN, as it can have a significant impact on the performance of the algorithm.\n",
    "- A small value of K will make the model more sensitive to noise in the data, while a large value of K may cause the model to lose the ability to capture local patterns in the data.\n",
    "\n",
    "#### Approach\n",
    "\n",
    "- One common approach to choosing the value of K is to use cross-validation to evaluate the performance of the model on a validation set for different values of K\n",
    "\n",
    "- Another approach is to use domain knowledge to choose a reasonable value of K based on the nature of the problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 3\n",
    "#### key differnces\n",
    "- KNN can be used for both classification and regression tasks, and the main difference between KNN classifier and KNN regressor is in the output they produce.\n",
    "\n",
    "### In KNN classification:\n",
    "The goal is to predict the class of a new data point based on its nearest neighbors in the training data. The output of the KNN classifier is a discrete class label that corresponds to the majority vote of the K nearest neighbors.\n",
    "\n",
    "### In KNN regression:\n",
    "The goal is to predict the continuous value of a new data point based on the values of its K nearest neighbors in the training data. The output of the KNN regressor is a continuous value that corresponds to the average or median value of the K nearest neighbors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 4\n",
    "The performance of KNN can be evaluated using various metrics depending on the type of problem being solved.\n",
    "\n",
    "#### For classification tasks:\n",
    "common evaluation metrics include accuracy, precision, recall, F1 score, and confusion matrix.\n",
    "\n",
    "##### For regression tasks: \n",
    "common evaluation metrics include mean squared error (MSE), mean absolute error (MAE), and R-squared (R2) score\n",
    "\n",
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 5\n",
    "The curse of dimensionality refers to the phenomenon where the performance of KNN degrades as the number of features or dimensions in the dataset increases\n",
    "\n",
    "\n",
    "-  In high-dimensional spaces, the distance between any two data points becomes more and more similar, making it difficult for KNN to differentiate between them. As a result, the algorithm requires a larger number of data points to be able to make accurate predictions, which can lead to overfitting or poor generalization performance. To mitigate the curse of dimensionality in KNN, various techniques such as feature selection, feature extraction, and dimensionality reduction can be used.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 6\n",
    "KNN is sensitive to missing values, as it relies on the distance between data points to make predictions.\n",
    "\n",
    "##### There are several methods for handling missing values in KNN, including:\n",
    "\n",
    "### Removing rows or columns with missing values:\n",
    " If the number of missing values is small, it may be possible to simply remove the rows or columns with missing values. However, this approach may result in a loss of information and may not be feasible if a large proportion of the data is missing.\n",
    "\n",
    "### Imputing missing values:\n",
    " Imputation involves replacing missing values with estimated values based on the values of other features in the dataset. There are several methods for imputing missing values, such as mean imputation, median imputation, KNN imputation, and regression imputation.\n",
    "\n",
    "### Treating missing values as a separate category: \n",
    "In some cases, missing values may be informative and may contain important information. In such cases, it may be possible to treat missing values as a separate category and include it as a feature in the model.\n",
    "\n",
    "\n",
    "\n",
    "------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 7\n",
    "The performance of KNN classifier and regressor depends on several factors such as the nature of the problem, the size of the dataset, and the value of the hyperparameter K. In general, KNN classifier works well for problems with a small number of classes and a large number of samples, while KNN regressor works well for problems with continuous target variables and a moderate to large number of samples.\n",
    "\n",
    "#### In terms of performance evaluation\n",
    " - KNN classifier is typically evaluated using accuracy, precision, recall, F1 score, and confusion matrix.\n",
    " - KNN regressor is typically evaluated using mean squared error (MSE), mean absolute error (MAE), and R-squared (R2) score.\n",
    "\n",
    " #### If the problem statement involves predicting a categorical or discrete variable\n",
    "\n",
    " - then KNN classifier would be more appropriate.\n",
    "\n",
    " ####  if the problem involves predicting a continuous variable\n",
    " -  KNN regressor would be more appropriate. \n",
    "\n",
    "\n",
    "\n",
    "  However, it is important to note that the choice between KNN classifier and regressor may also depend on other factors such as the distribution of the data, the nature of the features, and the complexity of the model. Therefore, it is recommended to try both variants and compare their performance on the specific problem being solved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 8\n",
    "\n",
    "### Strengths of KNN:\n",
    "\n",
    "- Simplicity: KNN is a simple and intuitive algorithm that is easy to implement and understand.\n",
    "\n",
    "- Non-parametric: KNN is a non-parametric algorithm, which means that it does not make any assumptions about the distribution of the data.\n",
    "\n",
    "- Robust to noisy data: KNN is robust to noisy data as it relies on the majority voting or averaging of the nearest neighbors, which helps to reduce the impact of outliers.\n",
    "\n",
    "### Weaknesses of KNN:\n",
    "\n",
    "- Computationally expensive: KNN can be computationally expensive, especially for large datasets, as it requires calculating the distance between each pair of data points.\n",
    "\n",
    "- Sensitive to the choice of hyperparameters: The performance of KNN depends on the choice of hyperparameters such as the number of neighbors K, which can be difficult to choose optimally.\n",
    "\n",
    "- Curse of dimensionality: As the number of dimensions or features in the dataset increases, the performance of KNN may suffer due to the curse of dimensionality, which refers to the sparsity of the data in high-dimensional spaces.\n",
    "\n",
    "#### Solutions to address weaknesses of KNN:\n",
    "- Computationally expensive: The computational cost of KNN can be reduced by using data structures such as KD-trees or ball trees, which can speed up the nearest neighbor search. Another approach is to use approximate nearest neighbor algorithms, which trade-off accuracy for speed.\n",
    "\n",
    "- Sensitive to the choice of hyperparameters: The choice of hyperparameters can be optimized using cross-validation or grid search, which involves testing different values of the hyperparameters on a validation set and selecting the values that give the best performance.\n",
    "\n",
    "- Curse of dimensionality: The curse of dimensionality can be mitigated by using techniques such as feature selection, feature extraction, or dimensionality reduction, which can help to reduce the number of dimensions and improve the performance of KNN.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 9\n",
    "Euclidean distance and Manhattan distance are two commonly used distance metrics in KNN algorithm for measuring the similarity between two data points\n",
    "\n",
    "### Euclidean distance\n",
    "Euclidean distance is calculated as the square root of the sum of squared differences between corresponding coordinates of two points. For example, if we have two points (x1, y1) and (x2, y2) in a two-dimensional space, then the Euclidean distance between them can be calculated as:\n",
    "\n",
    "##### <Center> sqrt((x2 - x1)^2 + (y2 - y1)^2)\n",
    "\n",
    "In general, Euclidean distance is useful when the data is dense and the scale of the features is important.\n",
    "\n",
    "\n",
    "### Manhattan Distance:\n",
    "Manhattan distance is the sum of the absolute value of the differences between corresponding coordinates of two points. For example, if we have two points (x1, y1) and (x2, y2) in  a two-dimensional space, then the Manhattan distance between them can be calculated as:\n",
    "\n",
    "#### <center> abs(x2 - x1) + abs(y2 - y1)\n",
    "In general, Manhattan distance is useful when the data is sparse and the scale of the features is less important."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 10\n",
    "Feature scaling plays an important role in KNN algorithm as it helps to ensure that all the features or variables in the dataset are treated equally and have a similar range of values. This is important because KNN is a distance-based algorithm, and the distance between two data points is influenced by the scale and range of the features\n",
    "\n",
    "### Feature Scaling Technique:\n",
    "we can use feature scaling techniques such as normalization or standardization to ensure that all the features have a similar range of values. Normalization scales the features to a range of 0 to 1, while standardization scales the features to have a mean of 0 and a standard deviation of 1.--etc\n",
    "\n",
    "\n",
    "#### Note:-\n",
    " By scaling the features, we can improve the performance of KNN by ensuring that all the features contribute equally to the distance metric, and reducing the impact of features with larger scales or ranges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
